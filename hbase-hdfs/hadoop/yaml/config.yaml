apiVersion: v1
data:
  core-site.xml: |-
    <configuration>
        <property>
            <name>fs.defaultFS</name>
            <value>hdfs://mycluster</value>
        </property>
        <property>
            <name>io.compression.codecs</name>
            <value>
                org.apache.hadoop.io.compress.GzipCodec,
                org.apache.hadoop.io.compress.DefaultCodec,
                com.hadoop.compression.lzo.LzoCodec,
                com.hadoop.compression.lzo.LzopCodec,
                org.apache.hadoop.io.compress.BZip2Codec
            </value>
        </property>
        <property>
            <name>io.compression.codec.lzo.class</name>
            <value>com.hadoop.compression.lzo.LzoCodec</value>
        </property>
        <property>
            <name>ha.zookeeper.quorum</name>
            <value>zoo1:2181</value>
        </property>
        <property>
            <name>ipc.server.tcpnodelay</name>
            <value>true</value>
        </property>
        <property>
            <name>ipc.client.tcpnodelay</name>
            <value>true</value>
        </property>
    </configuration>
  hdfs-site.xml: |-
    <configuration>
        <property>
            <name>dfs.nameservices</name>
            <value>mycluster</value>
        </property>
        <property>
            <name>dfs.ha.namenodes.mycluster</name>
            <value>nn0,nn1</value>
        </property>
        <property>
            <name>dfs.namenode.rpc-address.mycluster.nn0</name>
            <value>hadoop-namenode-0:8020</value>
        </property>
        <property>
            <name>dfs.namenode.rpc-address.mycluster.nn1</name>
            <value>hadoop-namenode-1:8020</value>
        </property>
        <property>
            <name>dfs.namenode.http-address.mycluster.nn0</name>
            <value>hadoop-namenode-0:50070</value>
        </property>
        <property>
            <name>dfs.namenode.http-address.mycluster.nn1</name>
            <value>hadoop-namenode-1:50070</value>
        </property>
        <property>
            <name>dfs.namenode.shared.edits.dir</name>
            <value>qjournal://hadoop-journalnode-0:8485;hadoop-journalnode-1:8485;hadoop-journalnode-2:8485/mycluster</value>
        </property>
        <property>
            <name>dfs.client.failover.proxy.provider.mycluster</name>
            <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
        </property>
        <property>
            <name>dfs.journalnode.edits.dir</name>
            <value>/home/hadoop/journaldata</value>
        </property>
        <property>
            <name>dfs.ha.automatic-failover.enabled</name>
            <value>true</value>
        </property>
        <property>
            <name>dfs.ha.fencing.methods</name>
            <value>shell(/bin/true)</value>
        </property>
        <property>
            <name>dfs.replication</name>
            <value>2</value>
        </property>
        <property>
            <name>dfs.permissions</name>
            <value>false</value>
        </property>
        <property>
            <name>dfs.name.dir</name>
            <value>file:///var/hdfs/name</value>
        </property>
        <property>
            <name>dfs.namenode.datanode.registration.ip-hostname-check</name>
            <value>false</value>
        </property>
        <property>
            <name>dfs.namenode.avoid.read.stale.datanode</name>
            <value>true</value>
        </property>
        <property>
            <name>dfs.namenode.avoid.write.stale.datanode</name>
            <value>true</value>
        </property>
        <property>
            <name>dfs.datanode.max.xcievers</name>
            <value>8192</value>
        </property>
        <property>
            <name>dfs.datanode.handler.count</name>
            <value>32</value>
        </property>
        <property>
            <name>dfs.datanode.data.dir</name>
            <value>/var/hdfs/data/datanode</value>
        </property>
       <property>
             <name>dfs.namenode.name.dir</name>
             <value>/var/hdfs/data/namenode</value>
       </property>
    </configuration>
  log4j.properties: |-
    log4j.rootLogger: INFO, stdout

    log4j.appender.stdout: org.apache.log4j.ConsoleAppender
    log4j.appender.stdout.layout: org.apache.log4j.PatternLayout
    log4j.appender.stdout.layout.ConversionPattern: %d{yyyy-MM-dd HH:mm:ss} %-5p %c{1}:%L - %m%n
  mapred-site.xml: |-
    <configuration>
    <property><name>mapreduce.framework.name</name><value>yarn</value></property>
    </configuration>
  yarn-site.xml: |-
    <configuration>
    <property><name>yarn.resourcemanager.hostname</name><value>namenode-0</value></property>
    <property><name>yarn.nodemanager.pmem-check-enabled</name><value>false</value></property>
    <property><name>yarn.nodemanager.delete.debug-delay-sec</name><value>600</value></property>
    <property><name>yarn.nodemanager.vmem-check-enabled</name><value>false</value></property>
    <property><name>yarn.nodemanager.aux-services</name><value>mapreduce_shuffle</value></property>
    <property><name>yarn.nodemanager.delete.thread-count</name><value>0</value></property>
    <property><name>yarn.nodemanager.local-dirs</name><value>/data/nodemanagerlocaldir</value></property>
    <property><name>yarn.resourcemanager.bind-host</name><value>0.0.0.0</value></property>
    <property><name>yarn.nodemanager.bind-host</name><value>0.0.0.0</value></property>
    <property><name>yarn.scheduler.maximum-allocation-mb</name><value>12000</value></property>
    <property><name>yarn.nodemanager.resource.memory-mb</name><value>12000</value></property>
    </configuration>
kind: ConfigMap
metadata:
  name: hadoopconf
  namespace: mu-architecture-demo